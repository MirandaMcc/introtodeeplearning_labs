{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Part2_debiasing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NDj7KBaW8Asz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MirandaMcc/introtodeeplearning_labs/blob/master/lab2/Part2_debiasing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag_e7xtTzT1W",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning_labs/blob/master/lab2/Part2_debiasing.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning_labs/blob/master/lab2/Part2_debiasing.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "\n",
        "# Laboratory 2: Computer Vision\n",
        "\n",
        "# Part 2: Debiasing Facial Detection Systems\n",
        "\n",
        "In the second portion of the lab, we'll explore two prominent aspects of applied deep learning: facial detection and algorithmic bias. \n",
        "\n",
        "Deploying fair, unbiased AI systems is critical to their long-term acceptance. Consider the task of facial detection: given an image, is it an image of a face? [Recent work from the MIT Media Lab](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) showed that this seemingly simple, but extremely important, task is subject to extreme amounts of algorithmic bias among select demographics. [Another report](https://ieeexplore.ieee.org/document/6327355) analyzed the face detection system used by the US law enforcement and found that it had significantly lower accuracy among dark skinned women between the age of 18-30 years old. \n",
        "\n",
        "Run the next code block for a short video from Google that explores how and why it's important to consider bias when thinking about machine learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQh5HZfbupFF",
        "colab_type": "code",
        "outputId": "4a6df36f-b4b5-442c-b735-0cce3ad31e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('59bMh59JQDo')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/59bMh59JQDo\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAUDBAgHBwgHBwcHBwgHCAcHBwgHCAgHBwgHBwcIBwcI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\n",
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fb3f15bff90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ezfc6Yv6IhI",
        "colab_type": "text"
      },
      "source": [
        "In this lab, we'll investigate [one recently published approach](http://introtodeeplearning.com/AIES_2019_Algorithmic_Bias.pdf) to addressing algorithmic bias. We'll build a facial detection model that learns the *latent variables* underlying face image datasets and uses this to adaptively re-sample the training data, thus mitigating any biases that may be present in order  to train a *debiased* model.\n",
        "\n",
        "Let's get started by installing the relevant dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E46sWVKK6LP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "3797d19a-6ac2-4632-eb15-21c10b042c7c"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "# Download the class repository\n",
        "! git clone https://github.com/aamini/introtodeeplearning_labs.git  > /dev/null 2>&1\n",
        "% cd introtodeeplearning_labs \n",
        "! git pull\n",
        "% cd .. \n",
        "\n",
        "# Import the necessary class-specific utility files for this lab\n",
        "import introtodeeplearning_labs as util"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/introtodeeplearning_labs\n",
            "Already up to date.\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1112 13:48:51.406275 139671468128128 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-70b8460b1af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Import the necessary class-specific utility files for this lab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mintrotodeeplearning_labs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/introtodeeplearning_labs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlab1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlab2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from lab3 import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/introtodeeplearning_labs/lab1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/introtodeeplearning_labs/lab1/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named regex",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0e77oOM3udR",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Datasets\n",
        "\n",
        "We'll be using three datasets in this lab. In order to train our facial detection models, we'll need a dataset of positive examples (i.e., of faces) and a dataset of negative examples (i.e., of things that are not faces). We'll use these data to train our models to classify images as either faces or not faces. Finally, we'll need a test dataset of face images. Since we're concerned about the potential *bias* of our learned models against certain demographics, it's important that the test dataset we use has equal representation across the demographics or features of interest. In this lab, we'll consider skin tone and gender. \n",
        "\n",
        "1.   **Positive training data**: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). A large-scale (over 200K images) of celebrity faces.   \n",
        "2.   **Negative training data**: [ImageNet](http://www.image-net.org/). Many images across many different categories. We'll take negative examples from a variety of non-human categories. \n",
        "3. **Test data**: [Pilot Parliaments Benchmark](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf) (PPB). The PPB dataset consists of images of 1270 male and female parliamentarians from various African and European countries and exhibits parity in both skin tone and gender. The gender of each face is annotated with the sex-based \"Male'' and \"Female'' labels. Skin tone annotations are based on the Fitzpatrick skin type classification system, with each image labeled as \"Lighter'' or \"Darker''.\n",
        "\n",
        "Let's begin by importing these datasets: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWXaaIWy6jVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b890b3bd-8268-4294-8bba-fbc14f58f950"
      },
      "source": [
        "# Get the training data: both images from CelebA and ImageNet\n",
        "path_to_training_data = tf.keras.utils.get_file('train_face.h5', 'https://www.dropbox.com/s/l5iqduhe0gwxumq/train_face.h5?dl=1')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.dropbox.com/s/l5iqduhe0gwxumq/train_face.h5?dl=1\n",
            "1263894528/1263889489 [==============================] - 1121s 1us/step\n",
            "1263902720/1263889489 [==============================] - 1121s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAY6pDc_Zljt",
        "colab_type": "text"
      },
      "source": [
        "This directly downloads the raw data. We've written two classes that do a bit of data pre-processing and import the results in a usable format: `TrainingDatasetLoader` for the training data and `PPBFaceEvaluator` for the test data.\n",
        "\n",
        "Let's create a `TrainingDatasetLoader` and use it to take a look at the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX-eUcEoazBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "7146cebe-b5c0-4a6c-aa7b-128b6100500b"
      },
      "source": [
        "# Instantiate a TrainingDatasetLoader using the downloaded dataset\n",
        "loader = util.TrainingDatasetLoader(path_to_training_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-059d189cc051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingDatasetLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_training_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIE321rxa_b3",
        "colab_type": "text"
      },
      "source": [
        "We can look at the size of the training dataset and grab a batch of size 100:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjPSjZZ_bGqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "4c9a24df-0063-4042-8a7d-bd0f90cb4be7"
      },
      "source": [
        "number_of_training_examples = loader.get_train_size()\n",
        "(images, labels) = loader.get_batch(100)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e49d17e4b2b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumber_of_training_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxtkJoqF6oH1",
        "colab_type": "text"
      },
      "source": [
        "Play around with displaying images to get a sense of what the training data actually looks like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg17jzwtbxDA",
        "colab_type": "code",
        "outputId": "5a42b007-ecdc-40d1-96ca-e70c57fe996a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "#@title Change the sliders to look at positive and negative training examples! { run: \"auto\" }\n",
        "\n",
        "face_images = images[np.where(labels==1)[0]]\n",
        "not_face_images = images[np.where(labels==0)[0]]\n",
        "\n",
        "idx_face = 20 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "idx_not_face = 25 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "plt.figure(figsize=(4,2))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(face_images[idx_face])\n",
        "plt.title(\"Face\")\n",
        "plt.grid(False)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(not_face_images[idx_not_face])\n",
        "plt.title(\"Not Face\")\n",
        "plt.grid(False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d7093e730d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mface_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnot_face_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0midx_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;31m#@param {type:\"slider\", min:0, max:50, step:1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f0d00c01df3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mface_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnot_face_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0midx_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;31m#@param {type:\"slider\", min:0, max:50, step:1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjuaEdLzb4pP",
        "colab_type": "text"
      },
      "source": [
        "We can also create a `PPBFaceEvaluator` instance for the PPB dataset and display some example images. We'll use this dataset later on in the evaluation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B4egQZY6wEt",
        "colab_type": "code",
        "outputId": "32f8ab86-ef6c-43cc-cb8a-d0ddd1073eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "#@title { run: \"auto\" }\n",
        "\n",
        "ppb = util.PPBFaceEvaluator() # create the dataset handler\n",
        "\n",
        "gender = \"female\" #@param [\"male\", \"female\"]\n",
        "skin_color = \"lighter\" #@param [\"lighter\", \"darker\"]\n",
        "\n",
        "img = ppb.get_sample_faces_from_demographic(gender, skin_color)\n",
        "plt.imshow(img)\n",
        "plt.grid(False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b51350668814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mppb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPPBFaceEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create the dataset handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"female\"\u001b[0m \u001b[0;31m#@param [\"male\", \"female\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mskin_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lighter\"\u001b[0m \u001b[0;31m#@param [\"lighter\", \"darker\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDj7KBaW8Asz",
        "colab_type": "text"
      },
      "source": [
        "### Thinking about bias\n",
        "\n",
        "Remember we'll be training our facial detection classifiers on the large, well-curated CelebA dataset (and ImageNet), and then evaluating their accuracy by testing them on the PPB dataset. Our goal is to build a model that trains on CelebA *and* achieves high classification accuracy on PPB across all demographics, and to thus show that this model does not suffer from any hidden bias. \n",
        "\n",
        "What exactly do we mean when we say a classifier is biased? In order to formalize this, we'll need to think about [*latent variables*](https://en.wikipedia.org/wiki/Latent_variable), variables that define a dataset but are not strictly observed. As defined in the generative modeling lecture, we'll use the term *latent space* to refer to the probability distributions of the aforementioned latent variables. Putting these ideas together, we consider a classifier *biased* if its classification decision changes after it sees some additional latent features. This notion of bias may be helpful to keep in mind throughout the rest of the lab. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIFDvU4w8OIH",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 CNN for facial detection \n",
        "\n",
        "First, we'll define and train a CNN on the facial classification task, and evaluate its accuracy on the PPB dataset. Later, we'll evaluate the performance of our debiased models against this baseline CNN. The CNN model has a relatively standard architecture consisting of a series of convolutional layers with batch normalization followed by two fully connected layers to flatten the convolution output and generate a class prediction. \n",
        "\n",
        "### Define and train the CNN model\n",
        "\n",
        "Like we did in the first part of the lab, we'll define our CNN model, and then train on the CelebA and ImageNet datasets using the `tf.GradientTape` class and the `tf.GradientTape.gradient` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82EVTAAW7B_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_outputs = 1 # number of outputs (i.e., face or not face)\n",
        "n_filters = 12 # base number of convolutional filters\n",
        "\n",
        "'''Function to define a standard CNN model'''\n",
        "def make_standard_classifier():\n",
        "    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "    Flatten = tf.keras.layers.Flatten\n",
        "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        Conv2D(filters=1*n_filters, kernel_size=[5,5],  strides=[2,2], input_shape=(64,64,3)),\n",
        "        BatchNormalization(),\n",
        "        \n",
        "        Conv2D(filters=2*n_filters, kernel_size=[5,5],  strides=[2,2]),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv2D(filters=4*n_filters, kernel_size=[3,3],  strides=[2,2]),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv2D(filters=6*n_filters, kernel_size=[3,3],  strides=[1,1]),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(1, activation=None),\n",
        "        tf.keras.layers.Dropout(0.5)\n",
        "    ])\n",
        "    return model\n",
        "  \n",
        "standard_classifier = make_standard_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-eWf3l_lCri",
        "colab_type": "text"
      },
      "source": [
        "Now let's train the standard CNN!\n",
        "\n",
        "**IMPORTANT NOTE: For educational purposes (to keep training times low), in this lab we are not training our models (both this CNN and a second model later on) for very long, and are only using a small, randomly sampled fraction of the CelebA dataset to train. As a result, the gap in accuracies between the standard CNN classifier and the debiased model (which you'll define soon) will not be as pronounced as if you were to train the models for longer and/or with more data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmOBzRgplB-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "a8fa3cfd-923c-4b59-8c6d-e415a737af8c"
      },
      "source": [
        "batch_size = 36\n",
        "num_epochs = 10  # keep small to run faster\n",
        "learning_rate = 1e-3\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # define our optimizer\n",
        "loss_history = util.LossHistory(smoothing_factor=0.99) # to record the evolution of the loss\n",
        "plotter = util.PeriodicPlotter(sec=2, scale='semilogy')\n",
        "\n",
        "# The training loop!\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  custom_msg = util.custom_progress_text(\"Epoch: %(epoch).0f Loss: %(loss)2.2f\")\n",
        "  bar = util.create_progress_bar(custom_msg)\n",
        "  \n",
        "  for idx in bar(range(loader.get_train_steps_per_epoch(batch_size))):\n",
        "    # First grab a batch of training data and convert the input images to tensors\n",
        "    x, y = loader.get_batch(batch_size)\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "    \n",
        "    # GradientTape to record differentiation operations\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = standard_classifier(x) # feed the images into the model\n",
        "      loss_value = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits) # compute the loss\n",
        "\n",
        "    custom_msg.update_mapping(epoch=epoch, loss=loss_value.numpy().mean())\n",
        "    # Backpropagation\n",
        "    grads = tape.gradient(loss_value, standard_classifier.variables)\n",
        "    optimizer.apply_gradients(zip(grads, standard_classifier.variables), global_step=tf.train.get_or_create_global_step())\n",
        "\n",
        "    loss_history.append(loss_value.numpy().mean()) \n",
        "    plotter.plot(loss_history.get())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3b61997f4062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# define our optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLossHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothing_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to record the evolution of the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplotter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPeriodicPlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'semilogy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKMdWVHeCxj8",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate performance of the standard CNN\n",
        "\n",
        "Next, let's evaluate the classification performance of our CelebA-trained standard CNN on the training dataset and the PPB dataset. For the PPB data, we'll look at the classification accuracy across four different demographics defined in PPB: dark-skinned male, dark-skinned female, light-skinned male, and light-skinned female.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "35-PDgjdWk6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "34e21d85-e415-42ba-b46c-71972ad789d5"
      },
      "source": [
        "# Evaluate on a subset of CelebA+Imagenet\n",
        "(batch_x, batch_y) = loader.get_batch(5000)\n",
        "y_pred_standard = tf.round(tf.nn.sigmoid(standard_classifier.predict(batch_x)))\n",
        "acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
        "print \"Standard CNN accuracy on (potentially biased) training set: {:.4f}\".format(acc_standard.numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-513a93261ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred_standard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandard_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0macc_standard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_standard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Standard CNN accuracy on (potentially biased) training set: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_standard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vfDD8ztGWk6x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "cba904f7-37f0-4fb2-8c6f-906a2d517e98"
      },
      "source": [
        "# Evaluate on PPB dataset (takes ~3 minutes)\n",
        "standard_cnn_accuracy = []\n",
        "for skin_color in ['lighter', 'darker']:\n",
        "  for gender in ['male', 'female']:\n",
        "    standard_cnn_accuracy.append( ppb.evaluate([standard_classifier], gender, skin_color, from_logit=True)[0] )\n",
        "    print \n",
        "    print \"{} {}: {}\".format(gender, skin_color, standard_cnn_accuracy[-1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ee2ae9d30993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mskin_color\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'lighter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'darker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mgender\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'female'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstandard_cnn_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mppb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstandard_classifier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskin_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"{} {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskin_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_cnn_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ppb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaPPGYdPmcCi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "fd87faaf-f879-4a37-b047-40323af45f28"
      },
      "source": [
        "plt.bar(range(4), standard_cnn_accuracy)\n",
        "plt.xticks(range(4), ('LM', 'LF', 'DM', 'DF'))\n",
        "plt.ylim(np.min(standard_cnn_accuracy)-0.01,np.max(standard_cnn_accuracy)+0.01)\n",
        "plt.ylabel('Accuracy')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-caeb4bcbd42e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_cnn_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'LM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandard_cnn_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandard_cnn_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mbar\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m                       mplDeprecation)\n\u001b[1;32m   2774\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2775\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2776\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2777\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1867\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[1;32m   2237\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/stride_tricks.pyc\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/stride_tricks.pyc\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYfAlqQmDL0lNGHxJauJ/Acz2XLpusNoKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Cvvt90DoAm",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the accuracies for this first model across these four groups. What do you observe? Would you consider this model biased or unbiased, and why? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLemS7dqECsI",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Variational autoencoder (VAE) for learning latent structure\n",
        "\n",
        "As you saw, the accuracy of the CNN varies across the four demographics we looked at. To think about why this may be, consider the dataset the model was trained on, CelebA. If certain features, such as dark skin or hats, are *rare* in CelebA, the model may end up biased against these as a result of training with a biased dataset. That is to say, its classification accuracy will be worse on faces that have under-represented features, such as dark-skinned faces or faces with hats, relevative to faces have features that are well-represented in the training data! This is a problem. \n",
        "\n",
        "Our goal is to train a *debiased* version of this classifier -- one that accounts for potential disparities in feature representation within the training data. Specifically, to build a debiased facial classifier, we'll train a model that learns a representation of the underlying latent space to the face training data. The model then uses this information to mitigate unwanted biases by sampling faces with rare features, like dark skin or hats, *more frequently* during training. The key design requirement for our model is that it can learn an *encoding* of the latent features in the face data in an entirely *unsupervised* way. To achieve this, we'll turn to variational autoencoders (VAEs).\n",
        "\n",
        "![The concept of a VAE](http://kvfrans.com/content/images/2016/08/vae.jpg)\n",
        "\n",
        "As shown in the schematic above, VAEs rely on an encoder-decoder structure to learn a latent representation of the input data. In the context of computer vision, the encoder network takes in input images, encodes them into a series of variables defined by a mean and standard deviation, and then draws from the distributions defined by these parameters to generate a set of sampled latent variables. The decoder network then \"decodes\" these variables to generate a reconstruction of the original image, which is used during training to help the model identify which latent variables are important to learn. \n",
        "\n",
        "Let's formalize two key aspects of the VAE model and define relevant functions for each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmbXKtcPkTXA",
        "colab_type": "text"
      },
      "source": [
        "### Understanding VAEs: loss function\n",
        "\n",
        "In practice, how can we train a VAE? In doing the reparameterization above, we constrain the means and standard deviations to approximately follow a unit Gaussian. Recall that these are learned parameters, and therefore must factor into the loss computation, and that the decoder portion of the VAE is using these parameters to output a reconstruction that should closely match the input image, which also must factor into the loss. What this means is that we'll have two terms in our VAE loss function:\n",
        "\n",
        "1.  **Latent loss ($L_{KL}$)**: measures how closely the learned latent variables match a unit Gaussian and is defined by the Kullback-Leibler (KL) divergence. Note that the reparameterization trick is what makes this loss function differentiable!\n",
        "2.   **Reconstruction loss ($L_{x}{(x,\\hat{x})}$)**: measures how accurately the reconstructed outputs match the input and is given by the $L^2$ norm of the input image and its reconstructed output.  \n",
        "\n",
        "The equations for both of these losses are provided below:\n",
        "\n",
        "$$ L_{KL}(\\mu, \\sigma) = \\frac{1}{2}\\sum\\limits_{j=0}^{k-1}\\small{(\\sigma_j + \\mu_j^2 - 1 - \\log{\\sigma_j})} $$\n",
        "\n",
        "$$ L_{x}{(x,\\hat{x})} = ||x-\\hat{x}||_2 $$ \n",
        "\n",
        "Thus for the VAE loss we have: \n",
        "\n",
        "$$ L_{VAE} = c\\cdot L_{KL} + L_{x}{(x,\\hat{x})} $$\n",
        "\n",
        "where $c$ is a weighting coefficient used for regularization. \n",
        "\n",
        "Now we're ready to define our VAE loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S00ASo1ImSuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate VAE loss given an input x, reconstructed output x_pred, \n",
        "#    encoded means mu, encoded log of standard deviation logsigma, and weight parameter for the latent loss\n",
        "def vae_loss_function(x, x_pred, mu, logsigma, kl_weight=0.0005):\n",
        "  '''TODO: Define the latent loss.\n",
        "  Hint: note that this function takes in logsigma!'''\n",
        "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + mu**2 - 1.0 - logsigma, axis=1) # TODO\n",
        "  '''TODO: Define the reconstruction loss. Hint: you'll need to use tf.reduce_mean to compute the mean-squared error, \n",
        "  with axis=(1,2,3)'''\n",
        "  reconstruction_loss = tf.reduce_mean((x-x_pred)**2, axis=(1,2,3))\n",
        "  '''TODO: Define the total VAE loss, i.e., weighted combination of two losses above.'''\n",
        "  total_vae_loss = (kl_weight*latent_loss) + reconstruction_loss\n",
        "  return total_vae_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8mpb3pJorpu",
        "colab_type": "text"
      },
      "source": [
        "Great! Now that we have a more concrete sense of how VAEs work, let's explore how we can leverage this network structure to train a *debiased* facial classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqtQH4S5fO8F",
        "colab_type": "text"
      },
      "source": [
        "### Understanding VAEs: reparameterization \n",
        "\n",
        "As you may recall from lecture, VAEs use a \"reparameterization  trick\" for sampling learned latent variables. Instead of the VAE encoder generating a single vector of real numbers for each latent variable, it generates a vector of means and a vector of standard deviations that are constrained to roughly follow Gaussian distributions. We then sample from the standard deviations and add back the mean to output this as our sampled latent vector. Formalizing this for a latent variable $z$ where we sample $\\epsilon \\sim \\mathcal{N}(0,(I))$ we have: \n",
        "\n",
        "$$ z = \\mathbb{\\mu} + e^{\\left(\\frac{1}{2} \\cdot \\log{\\Sigma}\\right)}\\circ \\epsilon $$\n",
        "\n",
        "where $\\mu$ is the mean and $\\Sigma$ is the covariance matrix. This is useful because it will let us neatly define the loss function for the VAE, generate randomly sampled latent variables, achieve improved network generalization, **and** make our complete VAE network differentiable so that it can be trained via backpropagation. Quite powerful!\n",
        "\n",
        "Let's define a function to implement the VAE sampling operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT6PGdNajl3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "# Arguments\n",
        "    args (tensor): mean and log of standard deviation of latent distribution (Q(z|X))\n",
        "# Returns\n",
        "    z (tensor): sampled latent vector\n",
        "\"\"\"\n",
        "def sampling(args):\n",
        "    z_mean, z_logsigma = args\n",
        "    batch = z_mean.shape[0]\n",
        "    dim = z_mean.shape[1]\n",
        "    \n",
        "    # by default, random_normal has mean=0 and std=1.0\n",
        "    epsilon = tf.random_normal(shape=(batch, dim))\n",
        "    '''TODO: Define the reparameterization computation using the equation in the text above.'''\n",
        "    z = z_mean + (tf.exp(.5*z_logsigma)*epsilon)\n",
        "    return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtHEYI9KNn0A",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Debiasing variational autoencoder (DB-VAE) for facial detection\n",
        "\n",
        "Now, we'll use the general idea behind the VAE architecture to build a model, termed a *debiasing variational autoencoder* or DB-VAE, to mitigate (potentially) unknown biases present within the training idea. We'll train our DB-VAE model on the facial detection task, run the debiasing operation during training, evaluate on the PPB dataset, and compare its accuracy to our original, biased CNN model.    \n",
        "\n",
        "### The DB-VAE model\n",
        "\n",
        "The key idea behind this debiasing approach is to use the latent variables learned via a VAE to adaptively re-sample the CelebA data during training. Specifically, we will alter the probability that a given image is used during training based on how often its latent features appear in the dataset. So, faces with rarer features (like dark skin, sunglasses, or hats) should become more likely to be sampled during training, while the sampling probability for faces with features that are over-represented in the training dataset should decrease (relative to uniform random sampling across the training data). \n",
        "\n",
        "A general schematic of the DB-VAE approach is shown here:\n",
        "\n",
        "![DB-VAE](https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab2/img/DB-VAE.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziA75SN-UxxO",
        "colab_type": "text"
      },
      "source": [
        "Recall that we want to apply our DB-VAE to a *supervised classification* problem -- the facial detection task. Importantly, note how the encoder portion in the DB-VAE architecture also outputs a single supervised variable, $z_o$, corresponding to the class prediction -- face or not face. Usually, VAEs are not trained to output any supervised variables (such as a class prediction)! This is another key distinction between the DB-VAE and a traditional VAE. \n",
        "\n",
        "Keep in mind that we only want to learn the latent representation of *faces*, as that's what we're ultimately debiasing against, even though we are training a model on a binary classification problem. We'll need to ensure that, **for faces**, our DB-VAE model both learns a representation of the unsupervised latent variables, captured by the distribution $q_\\phi(z|x)$, **and** outputs a supervised class prediction $z_o$, but that, **for negative examples**, it only outputs a class prediction $z_o$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XggIKYPRtOZR",
        "colab_type": "text"
      },
      "source": [
        "### Defining the DB-VAE loss function\n",
        "\n",
        "This means we'll need to be a bit clever about the loss function for the DB-VAE. The form of the loss will depend on whether it's a face image or a non-face image that's being considered. \n",
        "\n",
        "For **face images**, our loss function will have two components:\n",
        "\n",
        "\n",
        "1.   **VAE loss ($L_{VAE}$)**: consists of the latent loss and the reconstruction loss.\n",
        "2.   **Classification loss ($L_y(y,\\hat{y})$)**: standard cross-entropy loss for a binary classification problem. \n",
        "\n",
        "In contrast, for images of non-faces, our loss function is solely the classification loss. \n",
        "\n",
        "We can write a single expression for the loss by defining an indicator variable $\\mathcal{I}_f$which reflects which training data are images of faces ($\\mathcal{I}_f(x) = 1$ ) and which are images of non-faces ($\\mathcal{I}_f(x) = 0$). Using this, we obtain:\n",
        "\n",
        "$$L_{total} = L_y(y,\\hat{y}) + \\mathcal{I}_f(x)\\Big[L_{VAE}\\Big]$$\n",
        "\n",
        "Let's write a function to define the DB-VAE loss function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjieDs8Ovcqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function for DB-VAE\n",
        "def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
        "\n",
        "  '''TODO: call the relevant function that you created above to obtain the total VAE loss'''\n",
        "  vae_loss = vae_loss_function(x, x_pred, mu, logsigma, 0.0005)\n",
        "  '''TODO: define the classification loss for the binary classification task.'''\n",
        "  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit) # TODO\n",
        "  \n",
        "  # Use the training data labels to create variable face_mask\n",
        "  # This is I_f in the equation in the text above.\n",
        "  face_mask = tf.cast(tf.equal(y, 1), tf.float32)\n",
        "  \n",
        "  '''TODO: define the DB-VAE total loss, based on the equation in the text above.\n",
        "  Think about the dimensionality of your output and why tf.reduce_mean is being used'''\n",
        "  total_loss = tf.reduce_mean(classification_loss + (face_mask*vae_loss))\n",
        "  \n",
        "  return total_loss, classification_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIu_2LzNWwWY",
        "colab_type": "text"
      },
      "source": [
        "### DB-VAE architecture\n",
        "\n",
        "Now we're ready to define the DB-VAE architecture. First, let's define some key parameters for our model: the number of latent variables, the number of supervised outputs, and the starting number of filters for the first convolutional layer in the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds7o8AuFxUpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amYEnHdJxWYB",
        "colab_type": "text"
      },
      "source": [
        "To build the DB-VAE, we'll define each of the encoder and decoder networks separately, create and initialize the two models, and then construct the end-to-end VAE. We'll go through each of these steps in turn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k0tQeW1xpJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Define the encoder network for the DB-VAE'''\n",
        "def make_face_encoder_network():\n",
        "    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "    Flatten = tf.keras.layers.Flatten\n",
        "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=(64,64,3))\n",
        "    \n",
        "    hidden = Conv2D(filters=1*n_filters, kernel_size=[5,5],  strides=[2,2])(inputs)\n",
        "    hidden = BatchNormalization()(hidden)\n",
        "    hidden = Conv2D(filters=2*n_filters, kernel_size=[5,5],  strides=[2,2])(hidden)\n",
        "    hidden = BatchNormalization()(hidden)\n",
        "    hidden = Conv2D(filters=4*n_filters, kernel_size=[3,3],  strides=[2,2])(hidden)\n",
        "    hidden = BatchNormalization()(hidden)\n",
        "    hidden = Conv2D(filters=6*n_filters, kernel_size=[3,3],  strides=[1,1])(hidden)\n",
        "    hidden = BatchNormalization()(hidden)\n",
        "\n",
        "    hidden = Flatten(name='flatten')(hidden)\n",
        "#     hidden = Dense(128)(hidden)\n",
        "    \n",
        "    '''Encoder outputs:\n",
        "        y_logit: supervised class prediction\n",
        "        z_mean: means in the latent space\n",
        "        z_logsigma: standard deviations in the latent space'''\n",
        "    y_logit = Dense(1, activation=None, name='y_logit')(hidden)\n",
        "    z_mean = Dense(latent_dim, name='z_mean')(hidden)\n",
        "    z_logsigma = Dense(latent_dim, name='z_logsigma')(hidden)\n",
        "\n",
        "    # use reparameterization trick to sample from the latent space\n",
        "    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_logsigma])\n",
        "\n",
        "    # define the outputs that the encoder model should return\n",
        "    outputs = [y_logit, z_mean, z_logsigma, z]\n",
        "    # finalize the encoder model\n",
        "    encoder = tf.keras.Model(inputs=inputs, outputs=outputs, name='encoder')\n",
        "\n",
        "    # get the shape of the final convolutional output (right before the flatten)\n",
        "    flatten_layer_idx = encoder.layers.index(encoder.get_layer('flatten'))\n",
        "    pre_flatten_shape = encoder.layers[flatten_layer_idx-1].get_output_at(0).shape[1:]\n",
        "    \n",
        "    return encoder, inputs, outputs, pre_flatten_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlB-Gcot0gYw",
        "colab_type": "text"
      },
      "source": [
        "Similarly, we can define the decoder network, which takes as input the sampled latent variables, runs them through a series of deconvolutional layers, and outputs a reconstruction of the original input image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfWPHGrmyE7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Define the decoder network for the DB-VAE'''\n",
        "def make_face_decoder_network(pre_flatten_shape):\n",
        "  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
        "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "  Flatten = tf.keras.layers.Flatten\n",
        "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "  latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  \n",
        "#   hidden = Dense(128)(latent_inputs)\n",
        "  hidden = Dense(tf.reduce_prod(pre_flatten_shape))(latent_inputs)\n",
        "  hidden = tf.keras.layers.Reshape(pre_flatten_shape)(hidden)\n",
        "  \n",
        "  # series of deconvolutional layers with batch normalization\n",
        "  hidden = Conv2DTranspose(filters=4*n_filters, kernel_size=[3,3],  strides=[1,1])(hidden)\n",
        "  hidden = BatchNormalization()(hidden)\n",
        "  hidden = Conv2DTranspose(filters=2*n_filters, kernel_size=[3,3],  strides=[2,2])(hidden)\n",
        "  hidden = BatchNormalization()(hidden)\n",
        "  hidden = Conv2DTranspose(filters=1*n_filters, kernel_size=[5,5],  strides=[2,2])(hidden)\n",
        "  hidden = BatchNormalization()(hidden)\n",
        "  \n",
        "  x_hat = Conv2DTranspose(filters=3, kernel_size=[5,5], strides=[2,2])(hidden)\n",
        "\n",
        "  # instantiate decoder model\n",
        "  decoder = tf.keras.Model(inputs=latent_inputs, outputs=x_hat, name='decoder')\n",
        "  return decoder\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWCMu12w1BuD",
        "colab_type": "text"
      },
      "source": [
        "Now, call these functions to create the encoder and decoder!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSFDcFBL13c3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "64a68920-3074-485e-89c9-663a8606bd22"
      },
      "source": [
        "'''TODO: call the functions you defined above to create the encoder and decoder networks'''\n",
        "encoder, inputs, ouputs, pre_flatten_shape =  make_face_encoder_network()\n",
        "decoder = make_face_decoder_network(pre_flatten_shape)\n",
        "\n",
        "# initialize the models\n",
        "encoder_output = encoder(inputs)\n",
        "y_logit, z_mean, z_logsigma, z = encoder_output\n",
        "reconstructed_inputs = decoder(z)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ebfdb382de10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'''TODO: call the functions you defined above to create the encoder and decoder networks'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_flatten_shape\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmake_face_encoder_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_face_decoder_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_flatten_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# initialize the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-79c7e2cf6fc5>\u001b[0m in \u001b[0;36mmake_face_encoder_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# use reparameterization trick to sample from the latent space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_logsigma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# define the outputs that the encoder model should return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    848\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/layers/core.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m    787\u001b[0m       \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e7cff4cd444b>\u001b[0m in \u001b[0;36msampling\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# by default, random_normal has mean=0 and std=1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m'''TODO: Define the reparameterization computation using the equation in the text above.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz_logsigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/random_ops.pyc\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mshape_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mmean_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mstddev_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stddev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mshape_tensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    962\u001b[0m       \u001b[0;31m# not convertible to Tensors becasue of mixed content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    284\u001b[0m                                          as_ref=False):\n\u001b[1;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    263\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    264\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m   const_tensor = g.create_op(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    543\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    544\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <type 'tuple'> to Tensor. Contents: (None, 100). Consider casting elements to a supported type."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRI5_rz2Myy",
        "colab_type": "text"
      },
      "source": [
        "Finally we can construct our network end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WITL88Fm2Z0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "dde09b77-0ef1-4ed1-92ee-63b5a263fde4"
      },
      "source": [
        "# Construct the end to end vae\n",
        "vae = tf.keras.Model(inputs, reconstructed_inputs)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8e9f07f0e15d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjdyDDpc01ZZ",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize the architecture of the encoder to get a more concrete understanding of this network,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yKMwQU606ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "util.display_model(encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-clbYAj2waY",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the encoder architecture is virtually identical to the CNN from earlier in this lab. Note the outputs of this model: `y_logit, z_mean, z_logsigma, z`. Think carefully about why each of these are outputted and their significance to the problem at hand.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbDNlslgQc5A",
        "colab_type": "text"
      },
      "source": [
        "### Adaptive resampling for automated debiasing with DB-VAE\n",
        "\n",
        "So, how can we actually use DB-VAE to train a debiased facial detection classifier? Recall the DB-VAE architecture. As the input images are fed through the network, the encoder learns an estimate $\\mathcal{Q}(z|X)$ of the latent space. We want to increase the relative frequency of rare data by increased sampling of under-represented regions of the latent space. We can approximate $\\mathcal{Q}(z|X)$ using the frequency distributions of each of the learned latent variables, and then define the probability distribution of selecting a given datapoint $x$ based on this approximation. These probability distributions will be used during training to re-sample the data.\n",
        "\n",
        "You'll write a function to execute this update of the sampling probabilities, and then call this function within the DB-VAE training loop to actually debias the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fej5FDu37cf7",
        "colab_type": "text"
      },
      "source": [
        "First, we've defined a short helper function `get_latent_mu` that returns the latent variable means returned by the encoder after a batch of images is inputted to the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewWbf7TE7wVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to return the means for an input image batch\n",
        "def get_latent_mu(images, encoder, batch_size=1024):\n",
        "    N = images.shape[0]\n",
        "    mu = np.zeros((N, latent_dim))\n",
        "    for start_ind in xrange(0, N, batch_size):\n",
        "        end_ind = min(start_ind+batch_size, N+1)\n",
        "        batch = images[start_ind:end_ind]\n",
        "        batch = tf.convert_to_tensor(batch, dtype=tf.float32)/255.\n",
        "        _, batch_mu, _, _ = encoder(batch)\n",
        "        mu[start_ind:end_ind] = batch_mu\n",
        "    return mu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn4yK3SC72bo",
        "colab_type": "text"
      },
      "source": [
        "Now, let's define the actual resampling algorithm `get_training_sample_probabilities`. Importantly note the argument `smoothing_fac`. This parameter tunes the degree of debiasing: for `smoothing_fac=0`, the re-sampled training set will tend towards falling uniformly over the latent space. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiX9pmmC7_wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Function that recomputes the sampling probabilities for images within a batch\n",
        "    based on how they distribute across the '''\n",
        "def get_training_sample_probabilities(images, encoder, bins=10, smoothing_fac=0.0): \n",
        "    print \"Recomputing the sampling probabilities\"\n",
        "    \n",
        "    mu = get_latent_mu(images, encoder)\n",
        "    # sampling probabilities for the images\n",
        "    training_sample_p = np.zeros(mu.shape[0])\n",
        "    \n",
        "    # consider the distribution for each latent variable \n",
        "    for i in range(latent_dim):\n",
        "      \n",
        "        latent_distribution = mu[:,i]\n",
        "        '''TODO: generate a histogram of the latent distribution'''\n",
        "        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
        "\n",
        "        # find which latent bin every data sample falls in \n",
        "        # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.digitize.html\n",
        "        bin_edges[0] = -float('inf')\n",
        "        bin_edges[-1] = float('inf')\n",
        "        bin_idx = np.digitize(latent_distribution, bin_edges)\n",
        "\n",
        "        # smooth the density function (Eq. #4 in the DB-VAE paper)\n",
        "        hist_smoothed_density = hist_density + smoothing_fac\n",
        "        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
        "\n",
        "        p = 1.0/(hist_smoothed_density[bin_idx-1])\n",
        "        \n",
        "        # normalize all probabilities\n",
        "        p = p / np.sum(p)\n",
        "        \n",
        "        # update sampling probabilities \n",
        "        training_sample_p = np.maximum(p, training_sample_p)\n",
        "        \n",
        "    # final normalization\n",
        "    training_sample_p /= np.sum(training_sample_p)\n",
        "\n",
        "    return training_sample_p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF14fQkVUs-a",
        "colab_type": "text"
      },
      "source": [
        "Now that we've defined the resampling update, we can train our DB-VAE model on the CelebA/ImageNet training data, and run the above operation to re-weight the importance of particular data points as we train the model. Remember again that we only want to debias for features relevant to *faces*, not the set of negative examples.\n",
        "\n",
        "Complete the code block below to execute the training loop!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9YR8U43FVZ_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "29e27d7e-5fb0-432f-efb7-7e61c25b6ada"
      },
      "source": [
        "loss_history = []\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "enable_debiasing = True\n",
        "all_faces = loader.get_all_train_faces() # parameter from data loader\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  # progress message and bar\n",
        "  custom_msg = util.custom_progress_text(\"Epoch: %(epoch).0f   Iter: %(idx).0f   Class Loss: %(class_loss)2.2f   Loss: %(loss)2.2f\")\n",
        "  bar = util.create_progress_bar(custom_msg)\n",
        "\n",
        "  p_faces = None\n",
        "  if enable_debiasing: \n",
        "      # Recompute data sampling proabilities if debiasing is enabled\n",
        "      '''TODO: call the get_training_sample_probabilities function\n",
        "      to recompute the sampling probabilities when debiasing is enabled'''\n",
        "      p_faces = get_training_sample_probabilities(all_faces, encoder, 10, 0.0)\n",
        "  \n",
        "  for idx in bar(range(loader.get_train_steps_per_epoch(batch_size))):\n",
        "    # load a batch of data\n",
        "    (x, y) = loader.get_batch(batch_size, p_pos=p_faces)\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "  \n",
        "    # define GradientTape for automatic differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_logit, mu, logsigma, z = encoder(x)\n",
        "      x_hat = decoder(z)\n",
        "      '''TODO: call the debiasing_loss_function to compute the loss'''\n",
        "      loss, class_loss = debiasing_loss_function(x,x_hat,y,y_logit,mu,logsigma)\n",
        "    \n",
        "    '''TODO: use the GradientTape.gradient method to compute the gradients\n",
        "    Hint: compute the gradient of the 'loss' with respect to the variables in the VAE\n",
        "    Hint: recall how this is done in the standard CNN training code block'''\n",
        "    grads = tape.gradient(loss,vae.variables) # TODO\n",
        "    # apply gradients to variables\n",
        "    optimizer.apply_gradients(zip(grads, vae.variables),\n",
        "                              global_step=tf.train.get_or_create_global_step())\n",
        "\n",
        "    # track the losses\n",
        "    class_loss_value = class_loss.numpy().mean()\n",
        "    loss_value = loss.numpy().mean()\n",
        "    loss_history.append((class_loss_value, loss_value))\n",
        "    custom_msg.update_mapping(epoch=epoch, idx=idx, loss=loss_value, class_loss=class_loss_value)\n",
        "    \n",
        "    # plot the progress every 100 steps\n",
        "    if idx%100 == 0: \n",
        "      util.plot_sample(x,y,vae)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8a5f5f9cb844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menable_debiasing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_train_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# parameter from data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZBlWDPOVcHg",
        "colab_type": "text"
      },
      "source": [
        "Wonderful! Now we should have a trained and (hopefully!) debiased facial classification model, ready for evaluation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo34xC7MbaiQ",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Evaluation on Pilot Parliaments Benchmark (PPB) Dataset\n",
        "\n",
        "Finally let's test our DB-VAE model on the[ PPB dataset](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf). \n",
        "\n",
        "We'll evaluate both the overall accuracy of the DB-VAE as well as its accuracy on each the \"Dark Male\", \"Dark Female\", \"Light Male\", and \"Light Female\" demographics, and compare the performance of this debiased model against the biased CNN from earlier in the lab. \n",
        "\n",
        "Here are some example images from the PPB dataset.\n",
        "![PPB Example Images](https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab2/img/PPB%20faces.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruzxwzo2ko6N",
        "colab_type": "text"
      },
      "source": [
        "To assess performance, we'll measure the classification accuracy of each model, which we define as the fraction of PPB faces detected. By comparing the accuracy of a model without debiasing and our DB-VAE model, we can get a sense of how effectively we were able to debias against features like skin tone and gender.\n",
        "\n",
        "Let's evaluate our debiased model on the PPB test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgK77aB9oDtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "238f1c4c-4be7-4b68-dec4-3083af9c1995"
      },
      "source": [
        "# Evaluate on PPB dataset (takes ~4 minutes)\n",
        "accuracy_debiased = []\n",
        "for skin_color in ['lighter', 'darker']:\n",
        "  for gender in ['male', 'female']:\n",
        "    accuracy_debiased.append( ppb.evaluate([encoder], gender, skin_color, output_idx=0, from_logit=True)[0] )\n",
        "    print \n",
        "    print \"{} {}: {}\".format(gender, skin_color, accuracy_debiased[-1])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-2abbf9676a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mskin_color\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'lighter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'darker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mgender\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'female'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maccuracy_debiased\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mppb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskin_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"{} {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskin_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_debiased\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ppb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-3NzMB0oQtv",
        "colab_type": "text"
      },
      "source": [
        "We can calculate the accuracies of our model on the whole PPB dataset as well as across the four demographics proposed and visualize our results comparing to the standard, biased CNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzm-THVJkBjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "1d377128-cd1a-4abf-c468-dd8470d02ee1"
      },
      "source": [
        "bar_width = 0.3\n",
        "plt.bar(np.arange(4), standard_cnn_accuracy, width=bar_width)\n",
        "plt.bar(np.arange(4)+bar_width, accuracy_debiased, width=bar_width)\n",
        "plt.legend(('Standard Classifier','Debiased Classifier (DB-VAE)'))\n",
        "plt.xticks(np.arange(4), ('LM', 'LF', 'DM', 'DF'))\n",
        "plt.ylim(np.min([standard_cnn_accuracy,accuracy_debiased])-0.01,np.max([standard_cnn_accuracy,accuracy_debiased])+0.01)\n",
        "plt.ylabel('Accuracy')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-79ec551ad44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbar_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_cnn_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbar_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbar_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_debiased\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbar_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Standard Classifier'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Debiased Classifier (DB-VAE)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'LM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mbar\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m                       mplDeprecation)\n\u001b[1;32m   2774\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2775\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2776\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2777\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1867\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[1;32m   2237\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/stride_tricks.pyc\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/stride_tricks.pyc\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYfAlqQmDL0lNGHxJauJ/Acz2XLpusNoKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rESoXRPQo_mq",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Conclusion \n",
        "\n",
        "We encourage you to think about and maybe even address some questions raised by the approach and results outlined here:\n",
        "\n",
        "*  How does the accuracy of the DB-VAE across the four demographics compare to that of the standard CNN? Do you find this result surprising in any way?\n",
        "*  In which applications (either related to facial detection or not!) would debiasing in this way be desired? Are there applications where you may not want to debias your model? \n",
        "* Do you think it should be necessary for companies to demonstrate that their models, particularly in the context of tasks like facial detection, are not biased? If so, do you have thoughts on how this could be standardized and implemented?\n",
        "* Do you have ideas for other ways to address issues of bias, particularly in terms of the training data?\n",
        "\n",
        "Hopefully this lab has shed some light on a few concepts, from vision based tasks, to VAEs, to algorithmic bias. We like to think it has, but we're biased ;). \n",
        "\n",
        "![Faces](https://media1.tenor.com/images/44e1f590924eca94fe86067a4cf44c72/tenor.gif?itemid=3394328)"
      ]
    }
  ]
}